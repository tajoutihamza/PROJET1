{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Transactions Analysis Project\n",
    "\n",
    "This notebook analyzes credit card transactions data from India, including:\n",
    "1. Data Cleaning and Preprocessing\n",
    "2. Clustering Analysis (K-means, CAH, DBSCAN, GMM)\n",
    "3. Amount Prediction using Ensemble Methods\n",
    "4. Model Selection and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv('projet1.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "def handle_missing_values(df):\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Fill numerical missing values with median\n",
    "    numerical_columns = df_clean.select_dtypes(include=['int64', 'float64']).columns\n",
    "    for col in numerical_columns:\n",
    "        df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
    "    \n",
    "    # Fill categorical missing values with mode\n",
    "    categorical_columns = df_clean.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_columns:\n",
    "        df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Handle missing values\n",
    "df_clean = handle_missing_values(df)\n",
    "\n",
    "# Encode categorical variables and save encoders\n",
    "label_encoders = {}\n",
    "df_encoded = df_clean.copy()\n",
    "\n",
    "categorical_columns = df_clean.select_dtypes(include=['object']).columns\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_clean[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_encoded),\n",
    "    columns=df_encoded.columns\n",
    ")\n",
    "\n",
    "# Save preprocessors\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "joblib.dump(label_encoders, 'label_encoders.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering Analysis\n",
    "def evaluate_clustering(X, labels, algorithm_name):\n",
    "    from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "    \n",
    "    silhouette = silhouette_score(X, labels)\n",
    "    calinski = calinski_harabasz_score(X, labels)\n",
    "    \n",
    "    print(f\"\\n{algorithm_name} Metrics:\")\n",
    "    print(f\"Silhouette Score: {silhouette:.3f}\")\n",
    "    print(f\"Calinski-Harabasz Score: {calinski:.3f}\")\n",
    "\n",
    "# Apply clustering algorithms\n",
    "n_clusters = 5\n",
    "\n",
    "# K-means\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(df_scaled)\n",
    "evaluate_clustering(df_scaled, kmeans_labels, 'K-means')\n",
    "\n",
    "# Hierarchical Clustering\n",
    "hierarchical = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "hierarchical_labels = hierarchical.fit_predict(df_scaled)\n",
    "evaluate_clustering(df_scaled, hierarchical_labels, 'Hierarchical')\n",
    "\n",
    "# DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(df_scaled)\n",
    "evaluate_clustering(df_scaled, dbscan_labels, 'DBSCAN')\n",
    "\n",
    "# GMM\n",
    "gmm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
    "gmm_labels = gmm.fit_predict(df_scaled)\n",
    "evaluate_clustering(df_scaled, gmm_labels, 'GMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for prediction\n",
    "X = df_encoded.drop('Amount', axis=1)\n",
    "y = df_encoded['Amount']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate models\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"R2 Score: {r2:.3f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model = evaluate_model(rf_model, X_train, X_test, y_train, y_test, 'Random Forest')\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "xgb_model = evaluate_model(xgb_model, X_train, X_test, y_train, y_test, 'XGBoost')\n",
    "\n",
    "# AdaBoost\n",
    "ada_model = AdaBoostRegressor(random_state=42)\n",
    "ada_model = evaluate_model(ada_model, X_train, X_test, y_train, y_test, 'AdaBoost')\n",
    "\n",
    "# Save the best model (assuming XGBoost performs best)\n",
    "joblib.dump(xgb_model, 'credit_card_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
